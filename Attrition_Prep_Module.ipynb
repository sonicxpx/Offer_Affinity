{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "# The code was removed by Watson Studio for sharing."}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": "# Attrition Prep Module"}, {"cell_type": "markdown", "metadata": {}, "source": "This code needs to be run before executing notebook Model_Training_Deployment"}, {"cell_type": "markdown", "metadata": {}, "source": "Before executing this notebook on IBM Cloud, you need to:\n1) When you import this project on an IBM Cloud environment, a project access token should be inserted at the top of this notebook as a code cell.\nIf you do not see the cell above, Insert a project token: Click on More -> Insert project token in the top-right menu section and run the cell"}, {"cell_type": "markdown", "metadata": {}, "source": "**Sample Materials, provided under license. <br>\nLicensed Materials - Property of IBM. <br>\n\u00a9 Copyright IBM Corp. 2021. All Rights Reserved. <br>\nUS Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. <br>**"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "module_name= 'attrition_prep.py'"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Writing attrition_prep.py\n"}], "source": "%%writefile $module_name\n\n\"\"\"\nSample Materials, provided under license.\nLicensed Materials - Property of IBM\n\u00a9 Copyright IBM Corp. 2019. All Rights Reserved.\nUS Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nimport sys\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\nimport os\nimport json\n\nclass AttritionPrep():\n\n    def __init__(self, train_or_score, effective_date_earliest='2016-01-01', effective_date_latest='2018-09-30', effective_date='2018-09-30',\n                       columns_required=['CUSTOMER_CUSTOMER_ID', 'CUSTOMER_SUMMARY_END_DATE', 'CUSTOMER_SUMMARY_START_DATE',\n                                         'CUSTOMER_STATUS', 'CUSTOMER_SUMMARY_FUNDS_UNDER_MANAGEMENT', 'CUSTOMER_EFFECTIVE_DATE',\n                                         'CUSTOMER_RELATIONSHIP_START_DATE'],\n                       feature_attributes=['CUSTOMER_AGE_RANGE', 'CUSTOMER_GENDER', 'CUSTOMER_EDUCATION_LEVEL',\n                                         'CUSTOMER_URBAN_CODE', 'CUSTOMER_MARKET_GROUP', 'CUSTOMER_EMPLOYMENT_STATUS',\n                                         'CUSTOMER_MARITAL_STATUS', 'CUSTOMER_SUMMARY_ASSETS',\n                                         'CUSTOMER_SUMMARY_TOTAL_AMOUNT_OF_DEPOSITS', 'CUSTOMER_NUMBER_OF_DEPENDENT_ADULTS',\n                                         'CUSTOMER_NUMBER_OF_DEPENDENT_CHILDREN', 'CUSTOMER_ANNUAL_INCOME',\n                                         'CUSTOMER_SUMMARY_NUMBER_OF_ACCOUNTS', 'CUSTOMER_SUMMARY_TOTAL_AMOUNT_OF_ALL_FEES',\n                                         'CUSTOMER_SUMMARY_FUNDS_UNDER_MANAGEMENT', 'CUSTOMER_SUMMARY_AVERAGE_SENTIMENT_SCORE',\n                                         'CUSTOMER_INTERNET_BANKING_INDICATOR', 'CUSTOMER_FAMILY_SIZE',\n                                         'NUM_ACCOUNTS_WITH_RISK_TOLERANCE_HIGH', 'NUM_ACCOUNTS_WITH_RISK_TOLERANCE_LOW',\n                                         'NUM_ACCOUNTS_WITH_RISK_TOLERANCE_MODERATE', 'NUM_ACCOUNTS_WITH_RISK_TOLERANCE_VERY_LOW',\n                                         'NUM_ACCOUNTS_WITH_INVESTMENT_OBJECTIVE_GROWTH', 'NUM_ACCOUNTS_WITH_INVESTMENT_OBJECTIVE_INCOME',\n                                         'NUM_ACCOUNTS_WITH_INVESTMENT_OBJECTIVE_PLANNING', 'NUM_ACCOUNTS_WITH_INVESTMENT_OBJECTIVE_SECURE_GROWTH',\n                                         'NUM_ACCOUNTS_WITH_INVESTMENT_OBJECTIVE_SECURITY'],\n                       derive_column_list=[\"CUSTOMER_SUMMARY_TOTAL_AMOUNT_OF_DEPOSITS\", \"CUSTOMER_SUMMARY_FINANCIAL_ASSETS\",\n                                         \"CUSTOMER_SUMMARY_TOTAL_AMOUNT_OF_MARKET_CHANGE\", \"CUSTOMER_SUMMARY_NUMBER_OF_TRANSACTIONS\",\n                                         \"CUSTOMER_SUMMARY_FUNDS_UNDER_MANAGEMENT\", \"CUSTOMER_SUMMARY_NON_FINANCIAL_ASSETS\",\n                                         \"CUSTOMER_SUMMARY_NUMBER_OF_MOBILE_LOGINS\", \"CUSTOMER_SUMMARY_NUMBER_OF_LOGINS\"],\n                       granularity_key=\"CUSTOMER_CUSTOMER_ID\", target_attribute='TARGET', status_attribute='CUSTOMER_STATUS',\n                       funds_attribute='CUSTOMER_SUMMARY_FUNDS_UNDER_MANAGEMENT', date_customer_joined='CUSTOMER_RELATIONSHIP_START_DATE', \n                       customer_end_date='CUSTOMER_SUMMARY_END_DATE', customer_start_date='CUSTOMER_SUMMARY_START_DATE',\n                       period_attribute='CUSTOMER_TENURE', status_flag_attrition='Inactive', AUM_reduction_threshold=0.75,\n                       forecast_horizon=6, observation_window=6, sum_list=[\"CUSTOMER_SUMMARY_TOTAL_AMOUNT_OF_DEPOSITS\",\n                       \"CUSTOMER_SUMMARY_TOTAL_AMOUNT_OF_MARKET_CHANGE\", \"CUSTOMER_SUMMARY_NUMBER_OF_TRANSACTIONS\"], cat_threshold=10):\n\n        self.columns_required = columns_required\n        self.feature_attributes = feature_attributes\n        self.derive_column_list = derive_column_list \n        self.granularity_key = granularity_key\n        self.target_attribute = target_attribute\n        self.status_attribute = status_attribute\n        self.funds_attribute = funds_attribute\n        self.date_customer_joined = date_customer_joined \n        self.customer_end_date = customer_end_date   \n        self.customer_start_date = customer_start_date \n        self.period_attribute = period_attribute \n        self.status_flag_attrition = status_flag_attrition\n        self.train_or_score = train_or_score \n        self.AUM_reduction_threshold = AUM_reduction_threshold \n        self.effective_date_earliest = effective_date_earliest\n        self.effective_date_latest = effective_date_latest\n        self.forecast_horizon = forecast_horizon \n        self.observation_window = observation_window \n        self.effective_date = effective_date\n        self.sum_list = sum_list\n        self.cat_threshold = cat_threshold\n\n        # if effective date is a date convert it to a string for consistency\n        if self.train_or_score == 'score':\n            if isinstance(self.effective_date, datetime.datetime):\n                self.effective_date = datetime.datetime.strftime(self.effective_date, '%Y-%m-%d')\n        \n        if self.train_or_score == 'train':\n          # create a dictionary with all values for user inputs. We will save this out and use it for scoring\n          # to ensure that the user inputs are consistent across train and score notebooks\n          # exclude variables that won't be used for scoring\n          self.user_inputs_dict = { 'columns_required' : columns_required, 'feature_attributes' : feature_attributes,\n              'derive_column_list' : derive_column_list, 'granularity_key' : granularity_key, 'target_attribute' : target_attribute,\n              'status_attribute' : status_attribute, 'funds_attribute' : funds_attribute, 'date_customer_joined' : date_customer_joined,\n              'customer_end_date' : customer_end_date, 'customer_start_date' : customer_start_date, 'period_attribute' : period_attribute,\n              'status_flag_attrition' : status_flag_attrition, 'AUM_reduction_threshold' : AUM_reduction_threshold,\n              'forecast_horizon' : forecast_horizon, 'observation_window' : observation_window,\n              'sum_list' : sum_list, 'cat_threshold' : cat_threshold}\n\n    # function to get the difference between 2 dates returned in months\n    def udf_n_months(self, date1, date2):\n        month_dif = (relativedelta(date1, date2).months + \n        relativedelta(date1, date2).years * 12)\n        return month_dif\n    \n    # function subtracts a random_number * multiplier (months) from a given date\n    # the given date is changed so that it is the first of the month\n    def udf_sub_rand_latency(self, date1, randnumber, multiplier):\n        rand_cuttoff_month = (date1.replace(day=1) - relativedelta(months=int(randnumber * multiplier)))\n        return rand_cuttoff_month\n\n    def filter_attributes(self, df):\n        # this function takes in a dataframe and filters to include only the columns specified by the user\n        \n        default_attributes = self.columns_required + self.feature_attributes\n        # check to make sure we don't have duplicate columns names\n        default_attributes = list(set(default_attributes))\n\n        cols_passed_but_not_in_df = [attribute for attribute in default_attributes if attribute not in df.columns]\n        if len(cols_passed_but_not_in_df) > 0:\n            print(str(len(cols_passed_but_not_in_df)) + ' columns were passed but are not contained in the data. :' + str(cols_passed_but_not_in_df))\n            default_attributes = [col for col in default_attributes if col not in cols_passed_but_not_in_df]\n        df = df[default_attributes]\n        return df\n        \n    def fill_date_customer_joined(self, df):\n        # function to fill in any missing data for customer join date\n        # if only some records are missing for the customer and we have the join date in other records use that\n        # Otherwise, use the earliest customer summary start date\n\n        nb_cust_date_customer_joined_filled = df[df[self.date_customer_joined].isnull()][self.granularity_key].nunique()\n\n        if nb_cust_date_customer_joined_filled > 0:\n            print('Filling date_customer_joined for ' + str(nb_cust_date_customer_joined_filled) + ' customers')\n            # get a list of the customers who are missing start dates\n            cust_date_cust_joined_missing = list(df[df[self.date_customer_joined].isnull()][self.granularity_key].unique())\n\n            # check to see if any of the start date records for the customer are filled in\n            # use this if it's available\n            df_new_start_date = df[df[self.granularity_key].isin(cust_date_cust_joined_missing)].groupby(self.granularity_key)[self.date_customer_joined].min().reset_index()\n            df_new_start_date = df_new_start_date[df_new_start_date[self.date_customer_joined].notnull()]\n            df_new_start_date.rename(columns={self.date_customer_joined: 'MIN_START_DATE'}, inplace=True)\n            if df_new_start_date.shape[0] > 0:\n                df = df.merge(df_new_start_date, on=self.granularity_key, how='left')\n                df[self.date_customer_joined].fillna(df['MIN_START_DATE'], inplace=True)\n                # since these customers are not now missing start dates, remove them from the list\n                cust_date_cust_joined_missing = list(set(cust_date_cust_joined_missing) - set(df_new_start_date[self.granularity_key].unique()))\n                # drop the min_start_date var\n                df.drop('MIN_START_DATE', axis=1, inplace=True)\n\n            if len(cust_date_cust_joined_missing) > 0:\n                # get the earliest customer summary start date for each customer who is missing a start date\n                df_new_start_date = df[df[self.granularity_key].isin(cust_date_cust_joined_missing)].groupby(self.granularity_key)[self.customer_start_date].min().reset_index()\n                df_new_start_date.rename(columns={self.customer_start_date:'NEW_START_DATE'}, inplace=True)\n                # join back to original df and update \n                df = df.merge(df_new_start_date, on=self.granularity_key, how='left')\n                df[self.date_customer_joined].fillna(df['NEW_START_DATE'], inplace=True)\n                df.drop('NEW_START_DATE', axis=1, inplace=True)\n\n        return df\n    \n    def define_attrition_monthly(self, df):\n        \n        # defines an attrition flag for each month. If the customer was 'inactive' in the month,\n        # or their funds under management dropped by the threshold set or more, \n        # the customer gets a attrition flag of 1 for that month\n        \n        # if the customer hasn't a record for the previous month we just take the last month of data\n        # that they have, it's assumed any change in amount under management would have been recorded\n        # sort by customer ID and date so we have them in order\n        df = df.sort_values(by=[self.granularity_key, self.customer_end_date])\n        # add a new column which is the funds_attribute for the previous row (month in most cases)\n        df['AUM_PREV_MONTH'] = df.groupby([self.granularity_key])[self.funds_attribute].shift(1)\n        # get the ratio of this months funds vs previous month of data for the customer\n        df['RATIO'] = df[self.funds_attribute] / df['AUM_PREV_MONTH']\n        df['RATIO'].fillna(9999, inplace=True)\n        # flag records where drop is greater than allowed by threshold\n        df['FUNDS_DROP'] = 0\n        df.loc[df['RATIO']<=(1-df['AUM_reduction_threshold']), 'FUNDS_DROP'] = 1\n        # where funds_drop is 1 or the status attribute is 'Inactive' set attrition to 1, otherwise 0\n        df['attrition'] = 0\n        df.loc[((df['AUM_reduction_threshold']!=0) & (df['FUNDS_DROP']==1)) | (df[self.status_attribute]==self.status_flag_attrition),\n            'attrition'] = 1\n        # drop columns we don't need\n        df.drop('AUM_reduction_threshold', axis=1, inplace=True)\n        return df\n\n    def define_attrition_overall(self, df):\n        # This function flags all records associated with the customer with 1 if the customer experienced attrition in any month\n        \n        # group by customer ID, take max of attrition column, gives us a record per customer and tells us if they\n        # experienced attrition in any month in the dataset\n        df_cust_attrition = df.groupby(self.granularity_key)['attrition'].max().reset_index()\n        df_cust_attrition[self.target_attribute] = 0\n        df_cust_attrition.loc[df_cust_attrition['attrition']>=1, self.target_attribute] = 1\n        df_cust_attrition.drop('attrition', axis=1, inplace=True)\n        # join back to the original dataframe to give us our target\n        df = df.merge(df_cust_attrition, on=self.granularity_key, how='left')\n        \n        return df\n\n    def derive_features(self, df, col_list, observation_window):\n        # this function computes summary statistics of the customers numerical features over the observation window\n        # we calculate average, std, min, max, min max ratio, current vs average for numerical features\n\n        # filter to only include records that are within the observation window\n        df = df[df[self.customer_end_date]>=df['OBS_MONTH_MIN_OW']].copy()\n\n        # group by customer ID and calculate the summary stats\n        # first here is the first month in observation window\n        df_summary = df.groupby(self.granularity_key)[col_list].agg(['mean', 'std', 'min', 'max', 'last', 'first']).reset_index()\n        # rename the columns\n        df_summary.columns = df_summary.columns.map('_'.join)\n        # the above renames the customer id column, change it back to original\n        df_summary.rename(columns={self.granularity_key + '_': self.granularity_key}, inplace=True)\n        \n        for col in col_list:\n            df_summary[col + '_max_min_ratio'] = df_summary[col + '_max'] / df_summary[col + '_min']\n            df_summary[col + '_std_norm'] = df_summary[col + '_std'] / df_summary[col + '_mean']\n            df_summary[col + '_current_vs_mean'] = df_summary[col + '_last'] / df_summary[col + '_mean']\n            df_summary[col + '_current_vs_' + str(observation_window) + '_months_ago'] = df_summary[col + '_last'] / df_summary[col + '_first']\n\n        col_sum_list = [attribute for attribute in self.sum_list if attribute in col_list]\n\n        if len(col_sum_list) > 0:\n            df_for_summing = df.groupby(self.granularity_key)[col_sum_list].sum().reset_index()\n            for col in df_for_summing:\n                if col not in [self.granularity_key]:\n                    df_for_summing.rename(columns={col: col + '_sum'}, inplace=True)\n\n            df_summary = df_summary.merge(df_for_summing, on=self.granularity_key, how='left')\n\n        # we join back to the df that has a record for each month in the observation window for each customer\n        df = df.merge(df_summary, on=self.granularity_key, how='inner')\n\n        # remove the variables for first and last record\n        for col in col_list:    \n            # remove the _first and _last vars\n            df.drop(col + '_last', axis=1, inplace=True)\n            df.drop(col + '_first', axis=1, inplace=True)\n        \n        return df\n    \n    def data_cleaning(self, df, cat_threshold, train_or_score):\n        # remove categorical columns that have more than threshold number of levels - maybe group up instead?\n        # remove categorical columns that have only 1 value\n        str_cols = df.select_dtypes(include='object')\n        drop_cols = str_cols.columns[str_cols.nunique() > cat_threshold]\n        drop_cols = drop_cols.append(str_cols.columns[str_cols.nunique() == 1])\n        drop_cols = drop_cols.append(df.select_dtypes(include='datetime64[ns]').columns)\n\n        df = pd.get_dummies(df.drop(drop_cols, axis=1), drop_first=True)\n\n        # any column that starts with 'NUM_', replace null with 0 \n        # - this should be fixed with the new join at start and removed\n        num_col = [col for col in df if col.startswith('NUM_')]\n        df[num_col] = df[num_col].fillna(0)\n\n        if train_or_score == 'train':\n            # any column that has greater than 10% nulls is removed\n            # remove all columns that only have a constant value\n            for col in df.columns:\n                curr_col = df[col]\n                if (curr_col.isna().sum()/curr_col.shape[0]) > 0.1:\n                    df.drop(col, axis=1, inplace=True)\n                # if min = max the column is constant and should be dropped\n                elif curr_col.min() == curr_col.max():\n                    df.drop(col, axis=1, inplace=True)\n                \n        # any variable name ending in 'TIMESTAMP' is removed, as is any column that mentions 'ROW_ID'\n        # use upper here incase column names are proper or lower case\n        df = df[df.columns[~df.columns.str.upper().str.endswith('TIMESTAMP')]]\n        df = df[df.columns[~((df.columns.str.upper().str.contains('ROW_ID')) | (df.columns.str.upper().str.contains('ROWID')))]]\n\n        # remove the customer_id column\n        df.drop(self.granularity_key, axis=1, inplace=True)    \n        \n        return df\n    \n    def handle_inf_null_values(self, df):\n        # drop numeric columns with infinite values\n        max_vals = df.max()\n        cols_to_drop = list(max_vals[max_vals == np.inf].index)\n        df = df.drop(cols_to_drop, axis=1)\n        \n        # drop rows that have null values\n        df = df.dropna(how='any')\n        \n        return df\n    \n    def prep_data(self, df_raw, train_or_score):\n        np.random.seed(0)\n        # call the function to filter to only the selected attributes\n        df_raw = self.filter_attributes(df_raw)\n        \n        # fill missing customer join dates with the earliest customer start date\n        df_raw = self.fill_date_customer_joined(df_raw)\n\n        # order the dataframe by customer ID and customer_end_date\n        # makes it easier later on for selecting first and last records for a customer\n        df_raw = df_raw.sort_values(by=[self.granularity_key, self.customer_end_date])\n\n        # filter to include only records within the effective date range\n\n        wfd_max_customer_end_date = df_raw[self.customer_end_date].max()\n        wfd_min_customer_end_date = df_raw[self.customer_end_date].min()\n\n        # if training, use the dates specified in the input\n        # if scoring, use the effective_date for end point. Earliest date can be the earliest point in the data that we have\n        if train_or_score == 'train':\n            efd_latest = datetime.datetime.strptime(self.effective_date_latest, '%Y-%m-%d')\n            efd_earliest = datetime.datetime.strptime(self.effective_date_earliest, '%Y-%m-%d')\n        elif train_or_score == 'score':\n            efd_latest = datetime.datetime.strptime(self.effective_date, '%Y-%m-%d')\n            efd_earliest = wfd_min_customer_end_date\n\n        # remove records that are after our new latest date and before the new earliest date\n        df_raw = df_raw[(df_raw[self.customer_end_date]>=efd_earliest) & (df_raw[self.customer_end_date]<=efd_latest)]\n\n        print('Dataframe shape: ' + str(df_raw.shape))\n        print('Count of customers: ' + str(df_raw[self.granularity_key].nunique())) \n        \n        if df_raw[self.granularity_key].nunique() == 0:\n          print('No customers to score. Check effective date', file=sys.stderr)\n          return None\n\n        # add AUM_reduction_threshold column with the user defined threshold\n        df_raw['AUM_reduction_threshold'] = self.AUM_reduction_threshold\n\n        # call function to define attrition flag for each month\n        df_raw = self.define_attrition_monthly(df_raw)\n        # call the function that determines if the customer has attrited at any point in their tenure\n        df_raw = self.define_attrition_overall(df_raw)\n\n        attrited_df = df_raw[df_raw[self.target_attribute] == 1].copy()\n\n        print(str(attrited_df[self.granularity_key].nunique()) + ' customers attrited')\n\n        if train_or_score == 'train':    \n            # for each customer who attrited, get the month that they first attrited and select 1st day of the month\n            # subract a random number of months in [0, forecast_horizon] as a cutoff point\n            # ensures that attrition happened within horecast_horizon of cutoff date\n\n            # filter to include only months where attrition=1, get the first occurance per customer\n            df_attrited_first_occ = attrited_df[attrited_df['attrition']==1].groupby(self.granularity_key)[self.customer_end_date].min().reset_index()\n            # note that we pick the 1st of the month as the attrition happening\n            # create a random number to be used to select cutoff date before the attrition happened and within forecast_horizon\n            df_attrited_first_occ['RAND'] = np.random.rand(df_attrited_first_occ.shape[0])\n            df_attrited_first_occ['CUTOFF_DATE'] = df_attrited_first_occ.apply(lambda x: self.udf_sub_rand_latency(x[self.customer_end_date], x['RAND'], self.forecast_horizon), axis=1)\n            # drop columns that we don't need anymore\n            df_attrited_first_occ.drop([self.customer_end_date, 'RAND'], axis=1, inplace=True)\n            # the start of the observation window is observation_months before the cutoff_date\n            # get the start month of the observation window\n            df_attrited_first_occ['OBS_MONTH_MIN_OW'] = df_attrited_first_occ.apply(lambda x: x['CUTOFF_DATE'] - relativedelta(months=self.observation_window), axis=1)\n\n            # filter to only include records per customer from before their cutoff month\n            # if a customer has no records before the cutoff date, they are removed from the dataset\n            # join back to the original dataframe\n            attrited_df_b4_cutoff = df_raw.merge(df_attrited_first_occ, on=self.granularity_key, how='inner')\n            # filter to only include records before the cutoff date\n            attrited_df_b4_cutoff = attrited_df_b4_cutoff[attrited_df_b4_cutoff[self.customer_end_date]<attrited_df_b4_cutoff['CUTOFF_DATE']].copy()\n\n            # filter out any customer who does not have enough historical data before the cufoff date\n            # they must have at least observation_window number of months between first and last date of CUSTOMER_SUMMARY_END_DATE (customer_end_date)\n            attrited_cust_with_enough_history = attrited_df_b4_cutoff.groupby(self.granularity_key)[self.customer_end_date].agg([max, min]).reset_index()\n            attrited_cust_with_enough_history.columns = [self.granularity_key, 'MAX_DATE', 'MIN_DATE']\n            attrited_cust_with_enough_history['PERIODS_OF_DATA'] = attrited_cust_with_enough_history.apply(lambda x: self.udf_n_months(x['MAX_DATE'], x['MIN_DATE']), axis=1)\n            # remove anyone without enough observation window months\n            attrited_cust_with_enough_history = attrited_cust_with_enough_history[attrited_cust_with_enough_history['PERIODS_OF_DATA']>=self.observation_window]\n            # inner join back to attrited data to just leave customers who have enough data in observation window\n            attrited_df_b4_cutoff_filtered = attrited_df_b4_cutoff.merge(attrited_cust_with_enough_history, on=self.granularity_key, how='inner')\n            attrited_df_b4_cutoff_filtered.drop(['MAX_DATE', 'MIN_DATE', 'PERIODS_OF_DATA'], axis=1, inplace=True)\n            # next we derive features based on list passed\n            # the user can pass a specific list or use the default \n\n            # filter down the passed list to only include variables that we have in the dataset\n            col_list = [attribute for attribute in self.derive_column_list if attribute in attrited_df_b4_cutoff_filtered.columns]\n            print('Columns available for deriving features: ' +str(col_list))\n            # call the function to derive summary stats for each of the features passed in the list\n            attrited_df_features = self.derive_features(attrited_df_b4_cutoff_filtered, col_list, self.observation_window)\n\n            # keep only the latest month prior to the cutoff for each customer\n            # use last here in the event that there is more than 1 record for last date\n            attrited_df_prepped = attrited_df_features.groupby(self.granularity_key).last().reset_index()\n\n            print('Prepped data for ' + str(attrited_df_prepped.shape[0]) + ' customers')\n            print('Finished prepping for attrited customers')\n\n            # start prep for non attrited customers\n            # very similar to above but we select a random cutoff date \n            non_attrited_df = df_raw[df_raw[self.target_attribute] == 0].copy()\n\n            n_months = self.forecast_horizon + self.observation_window\n\n            # filter out customers with too little data or missing months\n            # compute a cutoff date that's randomly chosen \n\n            non_attrited_cust_with_enough_history = non_attrited_df.groupby(self.granularity_key)[self.customer_end_date].agg([max, min]).reset_index()\n            non_attrited_cust_with_enough_history.columns = [self.granularity_key, 'MAX_DATE', 'MIN_DATE']\n            non_attrited_cust_with_enough_history['PERIODS_OF_DATA'] = non_attrited_cust_with_enough_history.apply(lambda x: self.udf_n_months(x['MAX_DATE'], x['MIN_DATE']), axis=1)\n            # remove anyone that doesn't have observation_window + forecast_horizon months of data\n            non_attrited_cust_with_enough_history = non_attrited_cust_with_enough_history[non_attrited_cust_with_enough_history['PERIODS_OF_DATA']>=n_months]\n            non_attrited_cust_with_enough_history['RAND'] = np.random.rand(non_attrited_cust_with_enough_history.shape[0])\n            # select a random cutoff that will still give enough history and enough forecast_horizon\n            # ensure that the max date that can be selected still gives enough forecast_horizon (temp_date)\n            non_attrited_cust_with_enough_history['TEMP_DATE'] = non_attrited_cust_with_enough_history.apply(lambda x: x['MAX_DATE'] + relativedelta(months=1-self.forecast_horizon), axis=1)\n            non_attrited_cust_with_enough_history['CUTOFF_DATE'] = non_attrited_cust_with_enough_history.apply(lambda x: self.udf_sub_rand_latency(x['TEMP_DATE'], x['RAND'], x['PERIODS_OF_DATA'] - n_months), axis=1)\n            non_attrited_cust_with_enough_history.drop('TEMP_DATE', axis=1, inplace=True)\n            # join back to non_attrited_df and filter to save only records before the cutoff date\n            non_attrited_df_b4_cutoff_filtered = non_attrited_df.merge(non_attrited_cust_with_enough_history, on=self.granularity_key, how='inner')\n            non_attrited_df_b4_cutoff_filtered = non_attrited_df_b4_cutoff_filtered[non_attrited_df_b4_cutoff_filtered[self.customer_end_date]<non_attrited_df_b4_cutoff_filtered['CUTOFF_DATE']].copy()\n            # get the start month of the observation window\n            non_attrited_df_b4_cutoff_filtered['OBS_MONTH_MIN_OW'] = non_attrited_df_b4_cutoff_filtered.apply(lambda x: x['CUTOFF_DATE'] - relativedelta(months=self.observation_window), axis=1)\n            # drop the columns that we don't need\n            non_attrited_df_b4_cutoff_filtered.drop(['MAX_DATE', 'MIN_DATE', 'PERIODS_OF_DATA', 'RAND'], axis=1, inplace=True)\n\n            # derive additional features based on list passed in config\n            non_attrited_df_features = self.derive_features(non_attrited_df_b4_cutoff_filtered, col_list, self.observation_window)\n\n            # take the most recent date for each customer\n            # keep only the latest month prior to the cutoff for each customer\n            # use last here in the event that there is more than 1 record for last date\n            non_attrited_df_prepped = non_attrited_df_features.groupby(self.granularity_key).last().reset_index()\n\n            print('Prepped data for ' + str(non_attrited_df_prepped.shape[0]) + ' customers')\n            print('Finished prepping for non-attrited customers')\n\n            df_prepped = pd.concat([attrited_df_prepped, non_attrited_df_prepped])\n            # add new column for customer tenure\n            df_prepped[self.period_attribute] = df_prepped.apply(lambda x: self.udf_n_months(x[self.customer_end_date], x[self.date_customer_joined]), axis=1)\n            # drop columns that aren't needed \n            df_prepped.drop(['RATIO', 'FUNDS_DROP', 'attrition', 'OBS_MONTH_MIN_OW', 'AUM_PREV_MONTH'], axis=1, inplace=True)\n            # drop any column that looks like a date\n            for col in df_prepped.columns:\n                if df_prepped[col].dtype == 'datetime64[ns]':\n                    df_prepped.drop(col, axis=1, inplace=True)\n\n            # call the function for cleaning the data - call it here so that any columns that are removed\n            # are reflected in the json output\n            df_prepped = self.data_cleaning(df_prepped, self.cat_threshold, train_or_score)\n\n            # save out the user inputs \n            # and a list of columns that were created when the training dataset was built\n            self.user_inputs_dict['cols_used_for_training'] = list(df_prepped.columns)\n            with open('training_user_inputs_and_prepped_column_names.json', 'w') as f:\n                json.dump(self.user_inputs_dict, f)\n            \n            \n        elif train_or_score == 'score':\n\n            non_attrited_df = df_raw[df_raw[self.target_attribute] == 0].copy()\n\n            # filter out customers with too little data\n            # - they need to have at least observation_window months of data\n\n            non_attrited_cust_with_enough_history = non_attrited_df.groupby(self.granularity_key)[self.customer_end_date].agg([max, min]).reset_index()\n            non_attrited_cust_with_enough_history.columns = [self.granularity_key, 'MAX_DATE', 'MIN_DATE']\n            non_attrited_cust_with_enough_history['PERIODS_OF_DATA'] = non_attrited_cust_with_enough_history.apply(lambda x: self.udf_n_months(x['MAX_DATE'], x['MIN_DATE']), axis=1)\n            # filter to keep only those who have at least observation_window months of data\n            non_attrited_cust_with_enough_history = non_attrited_cust_with_enough_history[non_attrited_cust_with_enough_history['PERIODS_OF_DATA']>=self.observation_window]\n\n            print('We have ' + str(non_attrited_cust_with_enough_history.shape[0]) + ' customers to score')\n            if non_attrited_cust_with_enough_history.shape[0] == 0:\n                print('Error : No customer left for scoring. No customers had enough months of data', file=sys.stderr)\n                sys.exit(1)\n\n            # identify customers who do not have data for their last month\n            # - the same month as the effective date\n            # issue a warning but just use what data that we have anyway\n            missing_data_cust = non_attrited_cust_with_enough_history[non_attrited_cust_with_enough_history['MAX_DATE'].dt.strftime('%Y%m').astype(int)<int(efd_latest.strftime('%Y%m'))][self.granularity_key]\n\n            if len(list(missing_data_cust)) > 0:\n                print('There are ' + str(len(list(missing_data_cust))) + ' customers who do not have data up to the effective date.')\n\n            # keep only data in the observation window\n            # As it's possible that a customer does not have data up to the effective date,\n            # the observation window is still counted back from the effective date\n            # when scoring, the effective date is the last date of data that we have. \n            # Assuming it's the last day of the month, we add 1 day and subract observation_window months from it\n            # Gives us the first day of month for start of observation window\n            non_attrited_cust_with_enough_history['OBS_MONTH_MIN_OW'] = efd_latest + datetime.timedelta(days=1) - relativedelta(months=self.observation_window)\n            # join back to non_attrited_df\n            # and filter to include only records within the observation window\n            non_attrited_df_b4_cutoff_filtered = non_attrited_df.merge(non_attrited_cust_with_enough_history, on=self.granularity_key, how='inner')\n            non_attrited_df_b4_cutoff_filtered = non_attrited_df_b4_cutoff_filtered[non_attrited_df_b4_cutoff_filtered[self.customer_end_date]>=non_attrited_df_b4_cutoff_filtered['OBS_MONTH_MIN_OW']].copy()\n            if non_attrited_df_b4_cutoff_filtered.shape[0] == 0:\n                print('Error : No customer left for scoring. No customers had data within the observation window')\n                sys.exit(1)\n\n            # drop the columns that we don't need\n            non_attrited_df_b4_cutoff_filtered.drop(['MAX_DATE', 'MIN_DATE', 'PERIODS_OF_DATA'], axis=1, inplace=True)\n\n            col_list = [attribute for attribute in self.derive_column_list if attribute in non_attrited_df_b4_cutoff_filtered.columns]\n            # call the function to derive summary features\n            non_attrited_df_features = self.derive_features(non_attrited_df_b4_cutoff_filtered, col_list, self.observation_window)\n\n            non_attrited_df_features.drop('OBS_MONTH_MIN_OW', axis=1, inplace=True)\n            # take the most recent date for each customer\n            # keep only the latest month for each customer (the effective date month)\n            # use last here in the event that there is more than 1 record for last date\n            df_prepped = non_attrited_df_features.groupby(self.granularity_key).last().reset_index()\n            # add new column for customer tenure\n            df_prepped[self.period_attribute] = df_prepped.apply(lambda x: self.udf_n_months(x[self.customer_end_date], x[self.date_customer_joined]), axis=1)\n            # drop any column that looks like a date\n            for col in df_prepped.columns:\n                if df_prepped[col].dtype == 'datetime64[ns]':\n                    df_prepped.drop(col, axis=1, inplace=True)\n\n            # call the function to clean the data\n            df_prepped = self.data_cleaning(df_prepped, self.cat_threshold, train_or_score)\n\n            # Ensure that the dataset created for scoring has the same columns and order as the dataset used for training\n            # Read in the list of columns that were used for the training dataset\n            ##with open('/project_data/data_asset/training_user_inputs_and_prepped_column_names.json', 'r') as f:\n                ##user_inputs_df_cols_dict = json.load(f)\n            \n            ##cols_used_for_training = user_inputs_df_cols_dict['cols_used_for_training']\n\n            # don't need to include target variable for scoring\n            #cols_used_for_training.remove(self.target_attribute)\n\n            # if a column does not exist in scoring but is in training, add the column to scoring dataset\n            ##for col in cols_used_for_training:\n                ##if col not in list(df_prepped.columns):\n                    ##df_prepped[col] = 0\n\n            # if a column exists in scoring but not in training, delete it from scoring dataset\n            ##for col in list(df_prepped.columns):\n                ##if col not in cols_used_for_training:\n                    ##df_prepped.drop(col, axis=1, inplace=True)\n\n            # make sure order of scoring columns is same as training dataset\n            ##df_prepped = df_prepped[cols_used_for_training]\n\n            print('Finished prepping data for scoring')\n        \n        df_prepped = self.handle_inf_null_values(df_prepped)\n        return df_prepped\n"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "# Save to project\nf = open(module_name, 'r+b')\nproject.save_data(module_name, f, overwrite=True)\nf.close()\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3.9", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.13"}}, "nbformat": 4, "nbformat_minor": 1}